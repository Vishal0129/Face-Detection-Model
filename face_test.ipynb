{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image1 = cv2.imread(\"keanu.jpeg\")\n",
    "image2 = cv2.imread(\"murphy.jpeg\")\n",
    "face_location_image1 = face_recognition.face_locations(image1, model=\"hog\")\n",
    "face_location_image2 = face_recognition.face_locations(image2, model=\"hog\")\n",
    "\n",
    "known_faces = [\n",
    "    (face_recognition.face_encodings(image1, face_location_image1)[0], \"keanu\"),\n",
    "    (face_recognition.face_encodings(image2, face_location_image2)[0], \"murphy\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings, known_face_names = zip(*known_faces)\n",
    "# face_match = False\n",
    "# # obs_frames = queue.Queue()\n",
    "# obs_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces(image):\n",
    "    # global obs_frame, face_match\n",
    "    image = cv2.resize(image, (640, 480))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(image_rgb, model=\"hog\")\n",
    "    face_encodings = face_recognition.face_encodings(image_rgb, face_locations)\n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.5)\n",
    "        name = \"Person\"\n",
    "        if True in matches:\n",
    "            match_index = matches.index(True)\n",
    "            name = known_face_names[match_index]\n",
    "        top, right, bottom, left = face_location\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        cv2.putText(image, name, (left, top-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        \n",
    "    cv2.imshow(\"Frame\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_match' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39mrecognize_faces, args\u001b[39m=\u001b[39m(frame,))\u001b[39m.\u001b[39mstart()\n\u001b[0;32m     10\u001b[0m     frame_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mif\u001b[39;00m face_match:\n\u001b[0;32m     13\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m         cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mCamera Feed\u001b[39m\u001b[39m\"\u001b[39m, obs_frames\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'face_match' is not defined"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    if ret:\n",
    "        if frame_count % 10 == 0:\n",
    "            threading.Thread(target=recognize_faces, args=(frame,)).start()\n",
    "            frame_count = 0\n",
    "        \n",
    "        if face_match:\n",
    "            try:\n",
    "                cv2.imshow(\"Camera Feed\", obs_frames.pop(0))\n",
    "                # obs_frames.task_done()\n",
    "            except:\n",
    "                cv2.imshow(\"Camera Feed\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Camera not working\")\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_faces(image):\n",
    "    # Load the input image\n",
    "    # image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to RGB (face_recognition works with RGB images)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Find all face locations in the image\n",
    "    face_locations = face_recognition.face_locations(image_rgb, model=\"hog\")\n",
    "    \n",
    "    # Draw rectangles around the detected faces\n",
    "    for face_location in face_locations:\n",
    "        top, right, bottom, left = face_location\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the image with detected faces\n",
    "    cv2.imshow(\"Detected Faces\", cv2.resize(image, (800, 600)))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "try:\n",
    "    image_path = \"vishal.jpg\"\n",
    "    image = cv2.imread(image_path)\n",
    "    detect_faces(image)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"No faces were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "train_data_dir = 'C:/Users/saivishal radham/Desktop/Stuff/python/train_images/'  # Directory containing the labeled face images\n",
    "# validation_data_dir = 'path_to_validation_data'  # Directory containing the validation data\n",
    "new_face_dir = 'C:/Users/saivishal radham/Desktop/Stuff/python/new_face_dir/'  # Directory containing the new face image\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "class_labels = os.listdir(train_data_dir)\n",
    "num_classes = len(class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists('face_recognition_model.h5'):\n",
    "    model = tf.keras.models.load_model('face_recognition_model.h5')\n",
    "\n",
    "else:\n",
    "    # Load VGG16 model without the top classification layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "    # Freeze the base model layers to prevent them from being retrained\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom layers for face recognition\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)  # Adjust the size of the dense layer as per your requirements\n",
    "    output = Dense(num_classes, activation='softmax')(x)  # num_classes = number of faces to be recognized\n",
    "\n",
    "    # Create the final model\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a learning rate scheduler to reduce the learning rate during fine-tuning\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 5 == 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post7.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2970 sha256=eaca6bc7f3185b0358c48178ed192c45b1249406c7715891c04347572446df7d\n",
      "  Stored in directory: c:\\users\\saivishal radham\\appdata\\local\\pip\\cache\\wheels\\f3\\6d\\ba\\d999da6dff3d5cc6735e9855579fa236bf7c5bdbb210a873b8\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post7\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_generator, epochs\u001b[39m=\u001b[39;49mnum_epochs)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Save the updated model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mface_recognition_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\preprocessing\\image.py:2529\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   2485\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.preprocessing.image.apply_affine_transform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_affine_transform\u001b[39m(\n\u001b[0;32m   2487\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2499\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   2500\u001b[0m ):\n\u001b[0;32m   2501\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Applies an affine transformation specified by the parameters given.\u001b[39;00m\n\u001b[0;32m   2502\u001b[0m \n\u001b[0;32m   2503\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2527\u001b[0m \u001b[39m        ImportError: if SciPy is not available.\u001b[39;00m\n\u001b[0;32m   2528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2529\u001b[0m     \u001b[39mif\u001b[39;00m scipy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2530\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   2531\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mImage transformations require SciPy. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInstall SciPy.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2532\u001b[0m         )\n\u001b[0;32m   2534\u001b[0m     \u001b[39m# Input sanity checks:\u001b[39;00m\n\u001b[0;32m   2535\u001b[0m     \u001b[39m# 1. x must 2D image with one or more channels (i.e., a 3D tensor)\u001b[39;00m\n\u001b[0;32m   2536\u001b[0m     \u001b[39m# 2. channels must be either first or last dimension\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tune the model\n",
    "# model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator, epochs=num_epochs)\n",
    "\n",
    "# Save the updated model\n",
    "model.save('face_recognition_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 images belonging to 4 classes.\n",
      "Class Indices: {'keanu': 0, 'pavan': 1, 'vinay': 2, 'vishal': 3}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClass Indices:\u001b[39m\u001b[39m\"\u001b[39m, train_generator\u001b[39m.\u001b[39mclass_indices)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Check the shape of the images in the generator for debugging purposes\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_generator:\n\u001b[0;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch Shape:\u001b[39m\u001b[39m\"\u001b[39m, images\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLabel Shape:\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\preprocessing\\image.py:156\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\preprocessing\\image.py:168\u001b[0m, in \u001b[0;36mIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     index_array \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_generator)\n\u001b[0;32m    166\u001b[0m \u001b[39m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_batches_of_transformed_samples(index_array)\n",
      "File \u001b[1;32mc:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\preprocessing\\image.py:384\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator:\n\u001b[0;32m    383\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mget_random_transform(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 384\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_data_generator\u001b[39m.\u001b[39;49mapply_transform(x, params)\n\u001b[0;32m    385\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mstandardize(x)\n\u001b[0;32m    386\u001b[0m batch_x[i] \u001b[39m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\preprocessing\\image.py:2013\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[1;34m(self, x, transform_parameters)\u001b[0m\n\u001b[0;32m   2010\u001b[0m img_col_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2011\u001b[0m img_channel_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 2013\u001b[0m x \u001b[39m=\u001b[39m apply_affine_transform(\n\u001b[0;32m   2014\u001b[0m     x,\n\u001b[0;32m   2015\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtheta\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2016\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2017\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mty\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2018\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mshear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2019\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mzx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[0;32m   2020\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mzy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[0;32m   2021\u001b[0m     row_axis\u001b[39m=\u001b[39;49mimg_row_axis,\n\u001b[0;32m   2022\u001b[0m     col_axis\u001b[39m=\u001b[39;49mimg_col_axis,\n\u001b[0;32m   2023\u001b[0m     channel_axis\u001b[39m=\u001b[39;49mimg_channel_axis,\n\u001b[0;32m   2024\u001b[0m     fill_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfill_mode,\n\u001b[0;32m   2025\u001b[0m     cval\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcval,\n\u001b[0;32m   2026\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation_order,\n\u001b[0;32m   2027\u001b[0m )\n\u001b[0;32m   2029\u001b[0m \u001b[39mif\u001b[39;00m transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2030\u001b[0m     x \u001b[39m=\u001b[39m apply_channel_shift(\n\u001b[0;32m   2031\u001b[0m         x,\n\u001b[0;32m   2032\u001b[0m         transform_parameters[\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   2033\u001b[0m         img_channel_axis,\n\u001b[0;32m   2034\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saivishal radham\\Desktop\\Stuff\\python\\facerecog\\lib\\site-packages\\keras\\preprocessing\\image.py:2529\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   2485\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.preprocessing.image.apply_affine_transform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_affine_transform\u001b[39m(\n\u001b[0;32m   2487\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2499\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   2500\u001b[0m ):\n\u001b[0;32m   2501\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Applies an affine transformation specified by the parameters given.\u001b[39;00m\n\u001b[0;32m   2502\u001b[0m \n\u001b[0;32m   2503\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2527\u001b[0m \u001b[39m        ImportError: if SciPy is not available.\u001b[39;00m\n\u001b[0;32m   2528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2529\u001b[0m     \u001b[39mif\u001b[39;00m scipy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2530\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   2531\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mImage transformations require SciPy. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInstall SciPy.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2532\u001b[0m         )\n\u001b[0;32m   2534\u001b[0m     \u001b[39m# Input sanity checks:\u001b[39;00m\n\u001b[0;32m   2535\u001b[0m     \u001b[39m# 1. x must 2D image with one or more channels (i.e., a 3D tensor)\u001b[39;00m\n\u001b[0;32m   2536\u001b[0m     \u001b[39m# 2. channels must be either first or last dimension\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform fine-tuning\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Print the class indices for debugging purposes\n",
    "print(\"Class Indices:\", train_generator.class_indices)\n",
    "\n",
    "# Check the shape of the images in the generator for debugging purposes\n",
    "for images, labels in train_generator:\n",
    "    print(\"Batch Shape:\", images.shape)\n",
    "    print(\"Label Shape:\", labels.shape)\n",
    "    break\n",
    "\n",
    "# Define a learning rate scheduler to reduce the learning rate during fine-tuning\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 5 == 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "# Fine-tune the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator, epochs=num_epochs, callbacks=[LearningRateScheduler(lr_scheduler)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
